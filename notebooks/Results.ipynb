{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import seaborn as sns \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added to sys.path: c:\\Users\\antoi\\Documents\\MA3\\ML\\ML_course\\projects\\project2\\ML_Stroke_Project\n"
     ]
    }
   ],
   "source": [
    "# Add project root to sys.path\n",
    "project_root = os.path.abspath(\"../\")  # Adjust path if necessary\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Verify the path\n",
    "print(\"Project root added to sys.path:\", project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: loading of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that the preprocessing has already been done beforehand. You will find more informations regarding that in the report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading\n",
    "path_files=\"..\\data\\Raw_MissingDataImputed\"\n",
    "df_T1_=pd.read_excel(path_files+\"\\TiMeS_matrix_mdImputed_allT1.xlsx\")\n",
    "df_T2_=pd.read_excel(path_files+\"\\TiMeS_matrix_mdImputed_allT2.xlsx\")\n",
    "df_T3_=pd.read_excel(path_files+\"\\TiMeS_matrix_mdImputed_allT3.xlsx\")\n",
    "df_T4_=pd.read_excel(path_files+\"\\TiMeS_matrix_mdImputed_allT4.xlsx\")\n",
    "\n",
    "df_T1=df_T1_.copy()\n",
    "df_T2=df_T2_.copy()\n",
    "df_T3=df_T3_.copy()\n",
    "df_T4=df_T4_.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As some patients joined the study after the first session T1, and some left before T4, we aim there to take this fact in consideration. The goal is to group people from T1 and the ones that joined at T2, and people at T4 with the ones that left after T3. The reason behind that is that the duration between T1-T2 and T3-T4 is small (few weeks) comapred to the duration between T2 and T3 (6 months).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping T1-T2 vs T3-T4\n",
    "temp1=pd.concat((df_T1,df_T2[~df_T2[\"Patient\"].isin(df_T1[\"Patient\"])]))\n",
    "temp2=pd.concat((df_T4,df_T3[~df_T3[\"Patient\"].isin(df_T4[\"Patient\"])]))\n",
    "list_p = temp1[temp1[\"Patient\"].isin(temp2[\"Patient\"])][\"Patient\"]\n",
    "df_T1_T2_=temp1[temp1[\"Patient\"].isin(list_p)].reset_index(drop=True)\n",
    "df_T3_T4_=temp2[temp2[\"Patient\"].isin(list_p)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_T1_T2=df_T1_T2_.drop(columns=\"Patient\").dropna(axis=1).copy()\n",
    "df_T3_T4=df_T3_T4_[df_T1_T2.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     1\n",
       "2     1\n",
       "3     1\n",
       "4     1\n",
       "5     1\n",
       "6     1\n",
       "7     1\n",
       "8     1\n",
       "9     1\n",
       "10    1\n",
       "11    1\n",
       "12    1\n",
       "13    1\n",
       "14    1\n",
       "15    1\n",
       "16    1\n",
       "17    1\n",
       "18    1\n",
       "19    1\n",
       "20    0\n",
       "21    1\n",
       "22    1\n",
       "23    1\n",
       "24    1\n",
       "25    1\n",
       "26    1\n",
       "27    1\n",
       "28    1\n",
       "29    1\n",
       "30    1\n",
       "31    1\n",
       "32    1\n",
       "33    0\n",
       "34    0\n",
       "35    1\n",
       "36    1\n",
       "37    0\n",
       "38    1\n",
       "39    1\n",
       "40    1\n",
       "41    1\n",
       "42    1\n",
       "43    1\n",
       "44    0\n",
       "45    1\n",
       "46    1\n",
       "47    1\n",
       "48    1\n",
       "49    1\n",
       "50    1\n",
       "51    1\n",
       "52    1\n",
       "53    1\n",
       "54    1\n",
       "55    1\n",
       "56    0\n",
       "57    1\n",
       "58    1\n",
       "Name: Recovered, dtype: int32"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled=labelling(df_T3_T4)\n",
    "df_labeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st Part: Clustering approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_from_connectomes(path_T1, path_T3):\n",
    "    \"\"\"\n",
    "    Computes PCA features based on the relative difference of connectomes between T1 and T3.\n",
    "    Args:\n",
    "        path_T1 (str): Directory path for T1 connectomes.\n",
    "        path_T3 (str): Directory path for T3 connectomes.\n",
    "    Returns:\n",
    "        pd.DataFrame: PCA-transformed features with patient IDs.\n",
    "    \"\"\"\n",
    "    # Identify common patients\n",
    "    common_patients = get_common_patients(path_T1, path_T3)\n",
    "    \n",
    "    # Process T1 and T3 connectomes\n",
    "    connectomes_T1 = process_connectomes(path_T1, common_patients)\n",
    "    connectomes_T3 = process_connectomes(path_T3, common_patients)\n",
    "\n",
    "    # Compute relative difference (T1 - T3) / T1\n",
    "    patient_ids = []\n",
    "    flattened_matrices = []\n",
    "    for patient_id in common_patients:\n",
    "        values_T1 = np.array(connectomes_T1[patient_id])\n",
    "        values_T3 = np.array(connectomes_T3[patient_id])\n",
    "        values_T1[values_T1 == 0] = 1  # Avoid division by zero\n",
    "        relative_difference = (values_T1 - values_T3) / values_T1\n",
    "        patient_ids.append(patient_id)\n",
    "        flattened_matrices.append(relative_difference)\n",
    "\n",
    "    # Normalize the features\n",
    "    flattened_matrices = np.array(flattened_matrices)\n",
    "    scaler = StandardScaler()\n",
    "    normalized_features = scaler.fit_transform(flattened_matrices)\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=0.8)\n",
    "    pca_features = pd.DataFrame(pca.fit_transform(normalized_features))\n",
    "    pca_features[\"Patient\"] = patient_ids\n",
    "    pca_features[\"Patient\"]= pca_features[\"Patient\"].apply(lambda x: \"P\"+str(x))\n",
    "    return pca_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Get the temporal features, using (T1-T3)/T1,\n",
    "# for each patient present in both T1 and T3\n",
    "def get_df_temp(df1, df2, features):\n",
    "    df_tot = df1[df1.loc[:, \"Patient\"].isin(df2[\"Patient\"])].reset_index(drop=True)\n",
    "    A = df1[\"Patient\"].isin(df2[\"Patient\"])\n",
    "    list_patients = df1.loc[A.values, \"Patient\"].values\n",
    "    A = df1[df1[\"Patient\"].isin(list_patients)][features].reset_index(drop=True).copy()\n",
    "    B = df2[df2[\"Patient\"].isin(list_patients)][features].reset_index(drop=True).copy()\n",
    "    A_replaced = A.replace(0, 1)\n",
    "    feature_diff = (A - B) / A_replaced\n",
    "    df_tot[features] = feature_diff\n",
    "    return df_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preprocessing, with either min_max and z-standardization, \n",
    "def preprocess_temporal_data(df1, df2, df_raw, features, general_features, impute_strategy=\"mean\", normalization=None, use_general_features=True):\n",
    "    df_tot = get_df_temp(df1, df2, features)\n",
    "    df_tot = pd.merge(df_tot, df_raw, on=\"Patient\")\n",
    "    print(features)\n",
    "    print(general_features[1:])\n",
    "    if use_general_features:\n",
    "        all_features = general_features[1:] + features\n",
    "    else:\n",
    "        all_features = features\n",
    "\n",
    "    data_selected = df_tot[[\"Patient\"] + all_features].dropna(axis=0).reset_index(drop=True)\n",
    "    patients = data_selected[\"Patient\"]\n",
    "    data_features = data_selected.drop(columns=\"Patient\")\n",
    "    imputer = SimpleImputer(strategy=impute_strategy)#peut ete pas needed mais pca marche pas avec des NaN's\n",
    "    data_imputed = pd.DataFrame(imputer.fit_transform(data_features), columns=all_features)\n",
    "\n",
    "    if normalization == \"scaler\":\n",
    "        scaler = StandardScaler()\n",
    "        data_scaled = pd.DataFrame(scaler.fit_transform(data_imputed), columns=all_features)\n",
    "    elif normalization == \"min_max\":\n",
    "        scaler = MinMaxScaler()\n",
    "        data_scaled = pd.DataFrame(scaler.fit_transform(data_imputed), columns=all_features)\n",
    "    else:\n",
    "        data_scaled = data_imputed\n",
    "\n",
    "    data_scaled[\"Patient\"] = patients.values\n",
    "    return data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Clustering and output of the metrics for evaluation\n",
    "def cluster_and_evaluate(data, algorithm=\"kmeans\", params=None):\n",
    "    if algorithm == \"kmeans\":\n",
    "        model = KMeans(n_clusters=params.get(\"n_clusters\", 2), random_state=42)\n",
    "    elif algorithm == \"gmm\":\n",
    "        model = GaussianMixture(\n",
    "            n_components=params.get(\"n_clusters\", 2),\n",
    "            covariance_type=params.get(\"covariance_type\", \"full\"),\n",
    "            random_state=42\n",
    "        )\n",
    "    elif algorithm == \"hac\":\n",
    "        model = AgglomerativeClustering(n_clusters=params.get(\"n_clusters\", 2), linkage=params.get(\"linkage\", \"ward\"))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported algorithm\")\n",
    "\n",
    "    clusters = model.fit_predict(data)\n",
    "    silhouette = silhouette_score(data, clusters) if len(set(clusters)) > 1 else None\n",
    "    dbi = davies_bouldin_score(data, clusters) if len(set(clusters)) > 1 else None\n",
    "    ch_score = calinski_harabasz_score(data, clusters) if len(set(clusters)) > 1 else None\n",
    "    \n",
    "    return {\n",
    "        \"clusters\": clusters,\n",
    "        \"silhouette_score\": silhouette,\n",
    "        \"dbi\": dbi,\n",
    "        \"ch_score\": ch_score,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Visualize Clusters\n",
    "def visualize_clusters(data, clusters, idx, algorithm, preprocess):\n",
    "    \"\"\"\n",
    "    Visualize clusters using PCA with additional details in the title and labels.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): Data to visualize.\n",
    "        clusters (np.array): Cluster labels.\n",
    "        idx (str): Index for the pipeline step.\n",
    "        algorithm (str): The clustering algorithm used.\n",
    "        preprocess (str): The preprocessing method used.\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    data_reduced = pca.fit_transform(data)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(data_reduced[:, 0], data_reduced[:, 1], c=clusters, cmap=\"viridis\", s=50, alpha=0.7)\n",
    "    plt.colorbar(scatter, label=\"Cluster\")\n",
    "    plt.title(f\"PCA Visualization of Clusters\\nModel: {algorithm}, Preprocessing: {preprocess}, Step: {idx}\")\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Analyze Clusters\n",
    "def analyze_clusters(data, clusters, features):\n",
    "    data = data.copy()\n",
    "    data[\"Cluster\"] = clusters\n",
    "\n",
    "    # Only aggregate numeric columns\n",
    "    numeric_features = data[features].select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    summary = data.groupby(\"Cluster\")[numeric_features].agg([\"mean\", \"median\", \"std\"])\n",
    "    print(\"\\nCluster Summary Statistics:\\n\", summary)\n",
    "\n",
    "    # Plot feature distributions\n",
    "    for feature in numeric_features:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.boxplot(x=\"Cluster\", y=feature, data=data)\n",
    "        plt.title(f\"Feature Distribution by Cluster: {feature}\")\n",
    "        plt.xlabel(\"Cluster\")\n",
    "        plt.ylabel(feature)\n",
    "        plt.show()\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Main Pipeline\n",
    "def main_temporal_pipeline(df1, df2, df_general, features, general_features, \n",
    "                           algorithms, params, preprocesses, idx, paths_connectomes,\n",
    "                           use_pca_before_clustering=True,\n",
    "                           use_general_features=True, use_connectomes_features=True):\n",
    "    results = []\n",
    "    for algo in algorithms:\n",
    "        for preprocess in preprocesses:\n",
    "            df_temporal_features = get_df_temp(df1, df2, features)\n",
    "            df_general_filtered = df_general[df_general[\"Patient\"].isin(df_temporal_features[\"Patient\"])].reset_index(drop=True)\n",
    "            data_temporal_preprocessed = preprocess_temporal_data(\n",
    "                df1, df2, df_general_filtered, features, general_features, \"mean\", preprocess, \n",
    "                use_general_features=use_general_features,\n",
    "            )\n",
    "            if use_connectomes_features==True:\n",
    "                connectomes_features=compute_features_from_connectomes(paths_connectomes[0],paths_connectomes[1])\n",
    "                clustering_data = pd.merge(data_temporal_preprocessed, connectomes_features, on=\"Patient\", how=\"inner\")\n",
    "            clustering_data = data_temporal_preprocessed.drop(columns=[\"Patient\"])\n",
    "            if use_pca_before_clustering==True:\n",
    "                pca=PCA(n_components=0.9, random_state=42)\n",
    "                clustering_data=pca.fit_transform(clustering_data)\n",
    "            result = cluster_and_evaluate(clustering_data, algorithm=algo, params=params.get(algo, {}))\n",
    "            print(result)\n",
    "            if (result[\"silhouette_score\"] is not None and \n",
    "                result[\"dbi\"] is not None and \n",
    "                result[\"ch_score\"] is not None and \n",
    "                result[\"silhouette_score\"] > 0.2 and \n",
    "                result[\"dbi\"] < 3 and \n",
    "                result[\"ch_score\"] > 3):\n",
    "                \n",
    "                if use_general_features:\n",
    "                    analyzed_features = features + general_features[1:]\n",
    "                else:\n",
    "                    analyzed_features = features\n",
    "                \n",
    "                print(f\"\\nTesting {algo} with features: {analyzed_features} and preprocessing {preprocess}\")\n",
    "                visualize_clusters(clustering_data, result[\"clusters\"], idx, algo, preprocess)\n",
    "                #summary=analyze_clusters(data_temporal_preprocessed, result[\"clusters\"], analyzed_features)\n",
    "                \n",
    "                results.append({\"features\": analyzed_features, \"algorithm\": algo, \"metrics\": result})\n",
    "    \n",
    "    return results#, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algo and preprocessing\n",
    "\n",
    "algorithms = [\"kmeans\", \"gmm\", \"hac\"]\n",
    "preprocesses = [\"scaler\",\"min_max\"]\n",
    "params = {\n",
    "    \"kmeans\": {\"n_clusters\": 2},\n",
    "    \"gmm\": {\"n_clusters\": 2, \"covariance_type\": \"full\"},\n",
    "    \"hac\": {\"n_clusters\": 2, \"linkage\": \"complete\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing to get only the general features we want \n",
    "\n",
    "df_general = pd.get_dummies(df_general[general_features], columns=[\"Gender\",\"Affected_side\"], drop_first=True)\n",
    "df_general.rename(columns={\"Gender_2\": \"Is_Male\", \"Affected_side_2\": \"Is_Right_Side_Affected\"}, inplace=True)\n",
    "general_features = [\"Patient\", \"Age\", \"Is_Male\", \"Is_Right_Side_Affected\", \"Thrombolysis\"]\n",
    "df_general_final = df_general.groupby(\"Patient\").agg(\"mean\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing collinear features\n",
    "\n",
    "patients_T1=df_T1[\"Patient\"].values\n",
    "patients_T3=df_T3[\"Patient\"].values\n",
    "\n",
    "df_T1_corr=remove_collinear_features(df_T1.drop(columns=\"Patient\"),0.6)\n",
    "df_T3_corr=remove_collinear_features(df_T3.drop(columns=\"Patient\"),0.6)\n",
    "\n",
    "df_T1_corr[\"Patient\"]=patients_T1\n",
    "df_T3_corr[\"Patient\"]=patients_T3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modular approach: possibility to choose the categories of features for the clustering \n",
    "\n",
    "use_connectomes_features = True \n",
    "use_general_features = True  \n",
    "use_pca_before_clustering = True\n",
    "if len(df_T1_corr.columns)>len(df_T3_corr.columns):\n",
    "    features=df_T3_corr.drop(columns=\"Patient\").columns.tolist()\n",
    "else:\n",
    "    features=df_T1_corr.drop(columns=\"Patient\").columns.tolist()\n",
    "\n",
    "#features=motor_features+attention_features+executive_features+memory_features+sensory_features+language_features+neglect_features\n",
    "\n",
    "results = main_temporal_pipeline(df_T1, df_T3, df_general_final, features, general_features, \n",
    "                                 algorithms, params, preprocesses, \"T3-T1\", \n",
    "                                 paths_connectomes, use_pca_before_clustering=use_pca_before_clustering,\n",
    "                                 use_general_features=use_general_features,\n",
    "                                 use_connectomes_features=use_connectomes_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2st Part: Regression and classification approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The aim of this part is to first use a multi target regression to predict the T4 features from T1. We will add the patients from T2 and T3 as explained before, to get more data. Multiple models are trained and we compare their metrics. Multiples way of processing the data are also proposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature sets\n",
    "\n",
    "# Standarsized features\n",
    "X_std=Processing_std(df_T1_T2)\n",
    "Y_std=Processing_std(df_T3_T4)\n",
    "\n",
    "#Features that show the less collinearity\n",
    "X_corr=Processing_collinear(df_T1_T2)\n",
    "\n",
    "# Features after a Random forest with a treshold set at 0.02\n",
    "X_selected_rf, _ = Processing_rf_features(df_T1_T2,df_T3_T4)\n",
    "\n",
    "# Features after PCA reduction\n",
    "X_pca, explained_variance_X = Processing_PCA(df_T1_T2, n_components=0.8)\n",
    "\n",
    "# Features after Partial Least Squares\n",
    "X_std = Processing_std(df_T1_T2)  # Standardized features\n",
    "Y_std = Processing_std(df_T3_T4)  # Standardized targets\n",
    "\n",
    "Y_pls, pls_model = Processing_PLS(X_std, Y_std, n_components=5)\n",
    "\n",
    "\n",
    "feature_sets = {\n",
    "    #'AllFeatures': X_std,\n",
    "    'SelectedFeatures_RF': X_selected_rf,\n",
    "    'Reduced_X_PCA': X_pca,\n",
    "    'Non_collinear_features': X_corr\n",
    "}\n",
    "\n",
    "# Targets\n",
    "target_sets = {\n",
    "    'Original_Y': Y_std,\n",
    "    'Features_PLS': Y_pls\n",
    "}\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    'random_forest_model_mo' : random_forest_multi_out_model,\n",
    "    #'MSVR': msvr_model,\n",
    "    'RidgeRegression': ridge_multi_out_model,\n",
    "    'GradientBoosting': gradient_boost_multi_out_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating random_forest_model_mo with SelectedFeatures_RF and Original_Y...\n",
      "current metrics {'MAE': 0.7770820425075375, 'RMSE': 1.0983902299626476}\n",
      "Evaluating RidgeRegression with SelectedFeatures_RF and Original_Y...\n",
      "current metrics {'MAE': 0.8205712905943764, 'RMSE': 1.1449219946597657}\n",
      "Evaluating GradientBoosting with SelectedFeatures_RF and Original_Y...\n",
      "current metrics {'MAE': 0.7421691982814121, 'RMSE': 1.066791968930892}\n",
      "Evaluating random_forest_model_mo with SelectedFeatures_RF and Features_PLS...\n",
      "current metrics {'MAE': 2.0807327232940187, 'RMSE': 2.7349962108086445}\n",
      "Evaluating RidgeRegression with SelectedFeatures_RF and Features_PLS...\n",
      "current metrics {'MAE': 2.1364205700664884, 'RMSE': 2.855096722419211}\n",
      "Evaluating GradientBoosting with SelectedFeatures_RF and Features_PLS...\n",
      "current metrics {'MAE': 1.9760628978018708, 'RMSE': 2.5983205794409896}\n",
      "Evaluating random_forest_model_mo with Reduced_X_PCA and Original_Y...\n",
      "current metrics {'MAE': 0.7639917316774779, 'RMSE': 1.086310243470605}\n",
      "Evaluating RidgeRegression with Reduced_X_PCA and Original_Y...\n",
      "current metrics {'MAE': 0.7550125097454081, 'RMSE': 1.0637663252077358}\n",
      "Evaluating GradientBoosting with Reduced_X_PCA and Original_Y...\n",
      "current metrics {'MAE': 0.7360530731543173, 'RMSE': 1.0644846385039275}\n",
      "Evaluating random_forest_model_mo with Reduced_X_PCA and Features_PLS...\n",
      "current metrics {'MAE': 2.0850305753908263, 'RMSE': 2.684874544425016}\n",
      "Evaluating RidgeRegression with Reduced_X_PCA and Features_PLS...\n",
      "current metrics {'MAE': 1.9874660197539415, 'RMSE': 2.609091321600654}\n",
      "Evaluating GradientBoosting with Reduced_X_PCA and Features_PLS...\n",
      "current metrics {'MAE': 2.0353799724715196, 'RMSE': 2.6772810198404926}\n",
      "Evaluating random_forest_model_mo with Non_collinear_features and Original_Y...\n",
      "current metrics {'MAE': 0.7741328871040226, 'RMSE': 1.0904755632037952}\n",
      "Evaluating RidgeRegression with Non_collinear_features and Original_Y...\n",
      "current metrics {'MAE': 0.7224793451663178, 'RMSE': 1.0333994714168386}\n",
      "Evaluating GradientBoosting with Non_collinear_features and Original_Y...\n",
      "current metrics {'MAE': 0.7520862320919459, 'RMSE': 1.0728456668614865}\n",
      "Evaluating random_forest_model_mo with Non_collinear_features and Features_PLS...\n",
      "current metrics {'MAE': 2.0519426513017067, 'RMSE': 2.6762879726870796}\n",
      "Evaluating RidgeRegression with Non_collinear_features and Features_PLS...\n",
      "current metrics {'MAE': 1.8931694325255397, 'RMSE': 2.536234511220332}\n",
      "Evaluating GradientBoosting with Non_collinear_features and Features_PLS...\n",
      "current metrics {'MAE': 2.0031392713414027, 'RMSE': 2.614650876537876}\n",
      "                     Model                Features       Targets       MAE  \\\n",
      "13         RidgeRegression  Non_collinear_features    Original_Y  0.722479   \n",
      "7          RidgeRegression           Reduced_X_PCA    Original_Y  0.755013   \n",
      "8         GradientBoosting           Reduced_X_PCA    Original_Y  0.736053   \n",
      "2         GradientBoosting     SelectedFeatures_RF    Original_Y  0.742169   \n",
      "14        GradientBoosting  Non_collinear_features    Original_Y  0.752086   \n",
      "6   random_forest_model_mo           Reduced_X_PCA    Original_Y  0.763992   \n",
      "12  random_forest_model_mo  Non_collinear_features    Original_Y  0.774133   \n",
      "0   random_forest_model_mo     SelectedFeatures_RF    Original_Y  0.777082   \n",
      "1          RidgeRegression     SelectedFeatures_RF    Original_Y  0.820571   \n",
      "16         RidgeRegression  Non_collinear_features  Features_PLS  1.893169   \n",
      "5         GradientBoosting     SelectedFeatures_RF  Features_PLS  1.976063   \n",
      "10         RidgeRegression           Reduced_X_PCA  Features_PLS  1.987466   \n",
      "17        GradientBoosting  Non_collinear_features  Features_PLS  2.003139   \n",
      "15  random_forest_model_mo  Non_collinear_features  Features_PLS  2.051943   \n",
      "11        GradientBoosting           Reduced_X_PCA  Features_PLS  2.035380   \n",
      "9   random_forest_model_mo           Reduced_X_PCA  Features_PLS  2.085031   \n",
      "3   random_forest_model_mo     SelectedFeatures_RF  Features_PLS  2.080733   \n",
      "4          RidgeRegression     SelectedFeatures_RF  Features_PLS  2.136421   \n",
      "\n",
      "        RMSE  \n",
      "13  1.033399  \n",
      "7   1.063766  \n",
      "8   1.064485  \n",
      "2   1.066792  \n",
      "14  1.072846  \n",
      "6   1.086310  \n",
      "12  1.090476  \n",
      "0   1.098390  \n",
      "1   1.144922  \n",
      "16  2.536235  \n",
      "5   2.598321  \n",
      "10  2.609091  \n",
      "17  2.614651  \n",
      "15  2.676288  \n",
      "11  2.677281  \n",
      "9   2.684875  \n",
      "3   2.734996  \n",
      "4   2.855097  \n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for feature_name, X_features in feature_sets.items():\n",
    "    for target_name, Y_targets in target_sets.items():\n",
    "        for model_name, model_func in models.items():\n",
    "            print(f\"Evaluating {model_name} with {feature_name} and {target_name}...\")\n",
    "            metrics,_ = model_func(X_features, Y_targets)  # Cross-validation is applied inside\n",
    "            print(\"current metrics\",metrics)\n",
    "            results.append({\n",
    "                'Model': model_name,\n",
    "                'Features': feature_name,\n",
    "                'Targets': target_name,\n",
    "                'MAE': metrics['MAE'],\n",
    "                'RMSE': metrics['RMSE']\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame for better readability\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.sort_values(by='RMSE'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- THe ridge and gradient boosting regressions seem to have the best performances, when combined with the Non_collinear_features and the X features after PCA reduction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we can start classifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     1\n",
       "2     1\n",
       "3     1\n",
       "4     1\n",
       "5     1\n",
       "6     1\n",
       "7     1\n",
       "8     1\n",
       "9     1\n",
       "10    1\n",
       "11    1\n",
       "12    1\n",
       "13    1\n",
       "14    1\n",
       "15    1\n",
       "16    1\n",
       "17    1\n",
       "18    1\n",
       "19    1\n",
       "20    0\n",
       "21    1\n",
       "22    1\n",
       "23    1\n",
       "24    1\n",
       "25    1\n",
       "26    1\n",
       "27    1\n",
       "28    1\n",
       "29    1\n",
       "30    1\n",
       "31    1\n",
       "32    1\n",
       "33    0\n",
       "34    0\n",
       "35    1\n",
       "36    1\n",
       "37    0\n",
       "38    1\n",
       "39    1\n",
       "40    1\n",
       "41    1\n",
       "42    1\n",
       "43    1\n",
       "44    0\n",
       "45    1\n",
       "46    1\n",
       "47    1\n",
       "48    1\n",
       "49    1\n",
       "50    1\n",
       "51    1\n",
       "52    1\n",
       "53    1\n",
       "54    1\n",
       "55    1\n",
       "56    0\n",
       "57    1\n",
       "58    1\n",
       "Name: Recovered, dtype: int32"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Target\n",
    "y = labelling(df_T3_T4) \n",
    "\n",
    "# Preprocess data\n",
    "X_std = Processing_std(df_T1_T2)\n",
    "\n",
    "# Remove correlated features and standardize\n",
    "X_corr = Processing_collinear(df_T1_T2)\n",
    "\n",
    "# Handle imbalance\n",
    "X_balanced, y_balanced = Processing_imbalance(X_std, y)\n",
    "X_balanced_corr, y_balanced_corr = Processing_imbalance(X_corr, y)\n",
    "\n",
    "# Feature sets\n",
    "feature_sets = {\n",
    "    'Standardized_Features': X_std,\n",
    "    'Non_collinear_features': X_corr,\n",
    "    'Balanced_Data': X_balanced\n",
    "}\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    'LogisticRegression': logistic_regression_model,\n",
    "    'RandomForest': random_forest_model,\n",
    "    'GradientBoosting': gradient_boosting_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate combinations\n",
    "results = []\n",
    "for feature_name, X_features in feature_sets.items():\n",
    "    for model_name, model_func in models.items():\n",
    "        print(f\"Evaluating {model_name} with {feature_name}...\")\n",
    "        metrics, _ = model_func(X_features, y_balanced)\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Features': feature_name,\n",
    "            'Accuracy': metrics['Accuracy'],\n",
    "            'Precision': metrics['Precision'],\n",
    "            'Recall': metrics['Recall'],\n",
    "            'F1-Score': metrics['F1-Score']\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.sort_values(by='F1-Score', ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
