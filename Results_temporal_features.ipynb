{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 08/12 \n",
    "- Main Pipeline described with the preprocessing, the models used, the metrics obtained and the visualization\n",
    "- Compare the features values from T3 and T1 using (T1-T3)/T1, for all patients present in both T1 and T3\n",
    "- Use of the correlation matrix to remove features\n",
    "- Connectome features can also added\n",
    "- More general features (age, gender) can be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to remove the collinear features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_collinear_features(x, threshold):\n",
    "    '''\n",
    "    Objective:\n",
    "        Remove collinear features in a dataframe with a correlation coefficient\n",
    "        greater than the threshold. Removing collinear features can help a model \n",
    "        to generalize and improves the interpretability of the model.\n",
    "\n",
    "    Inputs: \n",
    "        x: features dataframe\n",
    "        threshold: features with correlations greater than this value are removed\n",
    "\n",
    "    Output: \n",
    "        dataframe that contains only the non-highly-collinear features\n",
    "    '''\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = x.corr()\n",
    "    iters = range(len(corr_matrix.columns) - 1)\n",
    "    drop_cols = []\n",
    "\n",
    "    # Iterate through the correlation matrix and compare correlations\n",
    "    for i in iters:\n",
    "        for j in range(i+1):\n",
    "            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n",
    "            col = item.columns\n",
    "            row = item.index\n",
    "            val = abs(item.values)\n",
    "\n",
    "            # If correlation exceeds the threshold\n",
    "            if val >= threshold:\n",
    "                # Print the correlated features and the correlation value\n",
    "                #print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n",
    "                drop_cols.append(col.values[0])\n",
    "\n",
    "    # Drop one of each pair of correlated columns\n",
    "    drops = set(drop_cols)\n",
    "    x = x.drop(columns=drops)\n",
    "    #print('Removed Columns {}'.format(drops))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to get the connectomes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_patients(path_T1, path_T3):\n",
    "    \"\"\"\n",
    "    Identifies common patients in T1 and T3 datasets.\n",
    "    Args:\n",
    "        path_T1 (str): Directory path for T1 connectomes.\n",
    "        path_T3 (str): Directory path for T3 connectomes.\n",
    "    Returns:\n",
    "        set: Set of patient IDs common to both T1 and T3.\n",
    "    \"\"\"\n",
    "    patients_T1 = set([f.split(\"_\")[0] for f in os.listdir(path_T1) if f.endswith(\".csv\")])\n",
    "    patients_T3 = set([f.split(\"_\")[0] for f in os.listdir(path_T3) if f.endswith(\".csv\")])\n",
    "    return patients_T1.intersection(patients_T3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_connectomes(path_dir, common_patients):\n",
    "    \"\"\"\n",
    "    Processes connectomes and filters for common patients.\n",
    "    Args:\n",
    "        path_dir (str): Directory path for connectomes.\n",
    "        common_patients (set): Set of common patient IDs.\n",
    "    Returns:\n",
    "        dict: Dictionary with patient IDs as keys and flattened connectomes as values.\n",
    "    \"\"\"\n",
    "    patient_connectomes = {}\n",
    "    for f in os.listdir(path_dir):\n",
    "        if f.endswith(\".csv\"):\n",
    "            patient_id = f.split(\"_\")[0]\n",
    "            if patient_id in common_patients:\n",
    "                file_path = os.path.join(path_dir, f)\n",
    "                connectome = pd.read_csv(file_path)\n",
    "                features_list = connectome.columns[1:-1]\n",
    "                connectome_values = connectome[features_list].values.flatten()\n",
    "                patient_connectomes[patient_id] = connectome_values\n",
    "    return patient_connectomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_from_connectomes(path_T1, path_T3):\n",
    "    \"\"\"\n",
    "    Computes PCA features based on the relative difference of connectomes between T1 and T3.\n",
    "    Args:\n",
    "        path_T1 (str): Directory path for T1 connectomes.\n",
    "        path_T3 (str): Directory path for T3 connectomes.\n",
    "    Returns:\n",
    "        pd.DataFrame: PCA-transformed features with patient IDs.\n",
    "    \"\"\"\n",
    "    # Identify common patients\n",
    "    common_patients = get_common_patients(path_T1, path_T3)\n",
    "    \n",
    "    # Process T1 and T3 connectomes\n",
    "    connectomes_T1 = process_connectomes(path_T1, common_patients)\n",
    "    connectomes_T3 = process_connectomes(path_T3, common_patients)\n",
    "\n",
    "    # Compute relative difference (T1 - T3) / T1\n",
    "    patient_ids = []\n",
    "    flattened_matrices = []\n",
    "    for patient_id in common_patients:\n",
    "        values_T1 = np.array(connectomes_T1[patient_id])\n",
    "        values_T3 = np.array(connectomes_T3[patient_id])\n",
    "        values_T1[values_T1 == 0] = 1  # Avoid division by zero\n",
    "        relative_difference = (values_T1 - values_T3) / values_T1\n",
    "        patient_ids.append(patient_id)\n",
    "        flattened_matrices.append(relative_difference)\n",
    "\n",
    "    # Normalize the features\n",
    "    flattened_matrices = np.array(flattened_matrices)\n",
    "    scaler = StandardScaler()\n",
    "    normalized_features = scaler.fit_transform(flattened_matrices)\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=0.8)\n",
    "    pca_features = pd.DataFrame(pca.fit_transform(normalized_features))\n",
    "    pca_features[\"Patient\"] = patient_ids\n",
    "    pca_features[\"Patient\"]= pca_features[\"Patient\"].apply(lambda x: \"P\"+str(x))\n",
    "    return pca_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Pipeline with all the steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Get the temporal features, using (T1-T3)/T1,\n",
    "# for each patient present in both T1 and T3\n",
    "def get_df_temp(df1, df2, features):\n",
    "    df_tot = df1[df1.loc[:, \"Patient\"].isin(df2[\"Patient\"])].reset_index(drop=True)\n",
    "    A = df1[\"Patient\"].isin(df2[\"Patient\"])\n",
    "    list_patients = df1.loc[A.values, \"Patient\"].values\n",
    "    A = df1[df1[\"Patient\"].isin(list_patients)][features].reset_index(drop=True).copy()\n",
    "    B = df2[df2[\"Patient\"].isin(list_patients)][features].reset_index(drop=True).copy()\n",
    "    A_replaced = A.replace(0, 1)\n",
    "    feature_diff = (A - B) / A_replaced\n",
    "    df_tot[features] = feature_diff\n",
    "    return df_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0bis: Compute PCA Features\n",
    "def compute_pca_features(data, n_components=0.8):\n",
    "    \"\"\"\n",
    "    Compute PCA features and append them to the dataset.\n",
    "    Args:\n",
    "        data (pd.DataFrame): Dataset to compute PCA on.\n",
    "        n_components (float): Variance ratio to retain.\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset with PCA features.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_transformed = pca.fit_transform(scaled_data)\n",
    "    pca_features = pd.DataFrame(pca_transformed, columns=[f\"PCA_Feature_{i+1}\" for i in range(pca_transformed.shape[1])])\n",
    "    return pca_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preprocessing, with either min_max and z-standardization, \n",
    "def preprocess_temporal_data(df1, df2, df_raw, features, general_features, impute_strategy=\"mean\", normalization=None, use_general_features=True):\n",
    "    df_tot = get_df_temp(df1, df2, features)\n",
    "    df_tot = pd.merge(df_tot, df_raw, on=\"Patient\")\n",
    "    print(features)\n",
    "    print(general_features[1:])\n",
    "    if use_general_features:\n",
    "        all_features = general_features[1:] + features\n",
    "    else:\n",
    "        all_features = features\n",
    "\n",
    "    data_selected = df_tot[[\"Patient\"] + all_features].dropna(axis=0).reset_index(drop=True)\n",
    "    patients = data_selected[\"Patient\"]\n",
    "    data_features = data_selected.drop(columns=\"Patient\")\n",
    "    imputer = SimpleImputer(strategy=impute_strategy)#peut ete pas needed mais pca marche pas avec des NaN's\n",
    "    data_imputed = pd.DataFrame(imputer.fit_transform(data_features), columns=all_features)\n",
    "\n",
    "    if normalization == \"scaler\":\n",
    "        scaler = StandardScaler()\n",
    "        data_scaled = pd.DataFrame(scaler.fit_transform(data_imputed), columns=all_features)\n",
    "    elif normalization == \"min_max\":\n",
    "        scaler = MinMaxScaler()\n",
    "        data_scaled = pd.DataFrame(scaler.fit_transform(data_imputed), columns=all_features)\n",
    "    else:\n",
    "        data_scaled = data_imputed\n",
    "\n",
    "    data_scaled[\"Patient\"] = patients.values\n",
    "    return data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Clustering and output of the metrics for evaluation\n",
    "def cluster_and_evaluate(data, algorithm=\"kmeans\", params=None):\n",
    "    if algorithm == \"kmeans\":\n",
    "        model = KMeans(n_clusters=params.get(\"n_clusters\", 2), random_state=42)\n",
    "    elif algorithm == \"gmm\":\n",
    "        model = GaussianMixture(\n",
    "            n_components=params.get(\"n_clusters\", 2),\n",
    "            covariance_type=params.get(\"covariance_type\", \"full\"),\n",
    "            random_state=42\n",
    "        )\n",
    "    elif algorithm == \"hac\":\n",
    "        model = AgglomerativeClustering(n_clusters=params.get(\"n_clusters\", 2), linkage=params.get(\"linkage\", \"ward\"))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported algorithm\")\n",
    "\n",
    "    clusters = model.fit_predict(data)\n",
    "    silhouette = silhouette_score(data, clusters) if len(set(clusters)) > 1 else None\n",
    "    dbi = davies_bouldin_score(data, clusters) if len(set(clusters)) > 1 else None\n",
    "    ch_score = calinski_harabasz_score(data, clusters) if len(set(clusters)) > 1 else None\n",
    "    \n",
    "    return {\n",
    "        \"clusters\": clusters,\n",
    "        \"silhouette_score\": silhouette,\n",
    "        \"dbi\": dbi,\n",
    "        \"ch_score\": ch_score,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Visualize Clusters\n",
    "def visualize_clusters(data, clusters, idx, algorithm, preprocess):\n",
    "    \"\"\"\n",
    "    Visualize clusters using PCA with additional details in the title and labels.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): Data to visualize.\n",
    "        clusters (np.array): Cluster labels.\n",
    "        idx (str): Index for the pipeline step.\n",
    "        algorithm (str): The clustering algorithm used.\n",
    "        preprocess (str): The preprocessing method used.\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    data_reduced = pca.fit_transform(data)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(data_reduced[:, 0], data_reduced[:, 1], c=clusters, cmap=\"viridis\", s=50, alpha=0.7)\n",
    "    plt.colorbar(scatter, label=\"Cluster\")\n",
    "    plt.title(f\"PCA Visualization of Clusters\\nModel: {algorithm}, Preprocessing: {preprocess}, Step: {idx}\")\n",
    "    plt.xlabel(\"PCA Component 1\")\n",
    "    plt.ylabel(\"PCA Component 2\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Analyze Clusters\n",
    "def analyze_clusters(data, clusters, features):\n",
    "    data = data.copy()\n",
    "    data[\"Cluster\"] = clusters\n",
    "\n",
    "    # Only aggregate numeric columns\n",
    "    numeric_features = data[features].select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    summary = data.groupby(\"Cluster\")[numeric_features].agg([\"mean\", \"median\", \"std\"])\n",
    "    print(\"\\nCluster Summary Statistics:\\n\", summary)\n",
    "\n",
    "    # Plot feature distributions\n",
    "    for feature in numeric_features:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.boxplot(x=\"Cluster\", y=feature, data=data)\n",
    "        plt.title(f\"Feature Distribution by Cluster: {feature}\")\n",
    "        plt.xlabel(\"Cluster\")\n",
    "        plt.ylabel(feature)\n",
    "        plt.show()\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Main Pipeline\n",
    "def main_temporal_pipeline(df1, df2, df_general, features, general_features, \n",
    "                           algorithms, params, preprocesses, idx, paths_connectomes,\n",
    "                           use_general_features=True, use_connectomes_features=True):\n",
    "    results = []\n",
    "    for algo in algorithms:\n",
    "        for preprocess in preprocesses:\n",
    "            df_temporal_features = get_df_temp(df1, df2, features)\n",
    "            df_general_filtered = df_general[df_general[\"Patient\"].isin(df_temporal_features[\"Patient\"])].reset_index(drop=True)\n",
    "            data_temporal_preprocessed = preprocess_temporal_data(\n",
    "                df1, df2, df_general_filtered, features, general_features, \"mean\", preprocess, \n",
    "                use_general_features=use_general_features,\n",
    "            )\n",
    "            if use_connectomes_features==True:\n",
    "                connectomes_features=compute_features_from_connectomes(paths_connectomes[0],paths_connectomes[1])\n",
    "                clustering_data = pd.merge(data_temporal_preprocessed, connectomes_features, on=\"Patient\", how=\"inner\")\n",
    "            clustering_data = data_temporal_preprocessed.drop(columns=[\"Patient\"])\n",
    "\n",
    "            result = cluster_and_evaluate(clustering_data, algorithm=algo, params=params.get(algo, {}))\n",
    "            print(result)\n",
    "            if (result[\"silhouette_score\"] is not None and \n",
    "                result[\"dbi\"] is not None and \n",
    "                result[\"ch_score\"] is not None and \n",
    "                result[\"silhouette_score\"] > 0.2 and \n",
    "                result[\"dbi\"] < 3 and \n",
    "                result[\"ch_score\"] > 3):\n",
    "                \n",
    "                if use_general_features:\n",
    "                    analyzed_features = features + general_features[1:]\n",
    "                else:\n",
    "                    analyzed_features = features\n",
    "                \n",
    "                print(f\"\\nTesting {algo} with features: {analyzed_features} and preprocessing {preprocess}\")\n",
    "                visualize_clusters(clustering_data, result[\"clusters\"], idx, algo, preprocess)\n",
    "                summary=analyze_clusters(data_temporal_preprocessed, result[\"clusters\"], analyzed_features)\n",
    "                \n",
    "                results.append({\"features\": analyzed_features, \"algorithm\": algo, \"metrics\": result})\n",
    "    \n",
    "    return results, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paths and files\n",
    "cd = os.getcwd()\n",
    "path_raw_data = os.path.join(cd, r\"data\\TiMeS_Raw_Data2023.xlsx\")\n",
    "file_paths = [\n",
    "    r\".\\data\\Raw_MissingDataImputed\\TiMeS_matrix_mdImputed_allT1.xlsx\",\n",
    "    r\".\\data\\Raw_MissingDataImputed\\TiMeS_matrix_mdImputed_allT2.xlsx\",\n",
    "    r\".\\data\\Raw_MissingDataImputed\\TiMeS_matrix_mdImputed_allT3.xlsx\",\n",
    "    r\".\\data\\Raw_MissingDataImputed\\TiMeS_matrix_mdImputed_allT4.xlsx\",\n",
    "]\n",
    "\n",
    "df_T1 = pd.read_excel(file_paths[0])\n",
    "df_T3 = pd.read_excel(file_paths[2])\n",
    "df_general = pd.read_excel(path_raw_data)\n",
    "\n",
    "path_T1_connectomes = os.path.join(cd, \"data\", \"Reduced_Connectomes\", \"ses-T1\")\n",
    "path_T3_connectomes = os.path.join(cd, \"data\", \"Reduced_Connectomes\", \"ses-T3\")\n",
    "paths_connectomes=[path_T1_connectomes,path_T3_connectomes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features\n",
    "#MAIS IL EN MANQUE PLEINS GENRE ILS SONT MARQU2S DANS LA DOC ET ILS SONT PAS DANS LES EXCELS PTN DUCOUP J'UTILISE \n",
    "#PAS CES LISTES MAIS BON AU CAS OU JE LES LAISSE\n",
    "\n",
    "general_features=[\"Patient\",\"Age\",\"Gender\",\"Affected_side\",\"Thrombolysis\"]\n",
    "motor_features = [\"Fugl.Meyer_affected_TOTAL\", \"BERG\", \"Ashworth_affected\", \"Purdue_affected_hand\"]\n",
    "attention_features=[\"TAP_alert_without_warning_RT\",\"TAP_single_condition_auditive_RT\",\"TAP_single_condition_visual_RT\"\n",
    "                    \"TAP_both_conditions_auditive_RT\",\"TAP_ both_conditions_visual_RT\",\n",
    "                    \"Bells_omissions_total\",\"CTM_A_time\"]\n",
    "executive_features=[\"Bimanual_coordination_corrected_total\",\"FAB_total\",\"AST_unaffected_total\",\"CERAD_copy_tota\"\n",
    "                    \"Stroop_interference_time\",\"Digit_backward_total\",\"Digit_sequencing_total\",\"Corsi_backward_total\",\n",
    "                    \"CTM_B_time\"]\n",
    "memory_features=[\"Digit_forward_total\",\"Corsi_forward total\"]\n",
    "sensory_features=[\"RASP_Total_unaffected\"]\n",
    "language_features=[\"Fluency_phon_final_score\",\"Fluency_sem_final_score\",\"LAST_TOTAL\"]\n",
    "neglect_features=[\"Line_bisection_20cm\",\"Line_bisection_5cm\",\"Bells_omission_LR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algo and preprocessing\n",
    "\n",
    "algorithms = [\"kmeans\", \"gmm\", \"hac\"]\n",
    "preprocesses = [\"scaler\",\"min_max\"]\n",
    "params = {\n",
    "    \"kmeans\": {\"n_clusters\": 2},\n",
    "    \"gmm\": {\"n_clusters\": 2, \"covariance_type\": \"full\"},\n",
    "    \"hac\": {\"n_clusters\": 2, \"linkage\": \"complete\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing to get only the general features we want \n",
    "\n",
    "df_general = pd.get_dummies(df_general[general_features], columns=[\"Gender\",\"Affected_side\"], drop_first=True)\n",
    "df_general.rename(columns={\"Gender_2\": \"Is_Male\", \"Affected_side_2\": \"Is_Right_Side_Affected\"}, inplace=True)\n",
    "general_features = [\"Patient\", \"Age\", \"Is_Male\", \"Is_Right_Side_Affected\", \"Thrombolysis\"]\n",
    "df_general_final = df_general.groupby(\"Patient\").agg(\"mean\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing collinear features\n",
    "\n",
    "patients_T1=df_T1[\"Patient\"].values\n",
    "patients_T3=df_T3[\"Patient\"].values\n",
    "\n",
    "df_T1_corr=remove_collinear_features(df_T1.drop(columns=\"Patient\"),0.6)\n",
    "df_T3_corr=remove_collinear_features(df_T3.drop(columns=\"Patient\"),0.6)\n",
    "\n",
    "df_T1_corr[\"Patient\"]=patients_T1\n",
    "df_T3_corr[\"Patient\"]=patients_T3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fugl.Meyer_affected_UPPER_EXTREMITY', 'Ashworth_affected', 'Fugl.Meyer_unaffected_UPPER_EXTREMITY', 'Fugl.Meyer_unaffected_WRIST', 'Fugl.Meyer_unaffected_HAND', 'Ashworth_unaffected', 'X9HPT_unaffected', 'RASP_TOTAL_affected', 'RASP_TOTAL_unaffected', 'MOCA_visuo.spatial.executive', 'MOCA_denomination', 'MOCA_attention_tot', 'MOCA_language_tot', 'MOCA_abstraction', 'MOCA_memory', 'MOCA_orientation', 'Fluency_sem_final_score', 'Bells_time', 'Bells_omissions_L.R', 'SIS_memory.thinking_capacities', 'SIS_mood.emotions', 'STAI_S', 'mRNLI_global', 'mRNLI_social', 'PSQI', 'MFI_motivation', 'SCS_Total']\n",
      "['Age', 'Is_Male', 'Is_Right_Side_Affected', 'Thrombolysis']\n",
      "{'clusters': array([0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "       1, 0, 0]), 'silhouette_score': 0.23829863859151337, 'dbi': 2.8053581287703766, 'ch_score': 4.843520943802616}\n",
      "\n",
      "Testing kmeans with features: ['Fugl.Meyer_affected_UPPER_EXTREMITY', 'Ashworth_affected', 'Fugl.Meyer_unaffected_UPPER_EXTREMITY', 'Fugl.Meyer_unaffected_WRIST', 'Fugl.Meyer_unaffected_HAND', 'Ashworth_unaffected', 'X9HPT_unaffected', 'RASP_TOTAL_affected', 'RASP_TOTAL_unaffected', 'MOCA_visuo.spatial.executive', 'MOCA_denomination', 'MOCA_attention_tot', 'MOCA_language_tot', 'MOCA_abstraction', 'MOCA_memory', 'MOCA_orientation', 'Fluency_sem_final_score', 'Bells_time', 'Bells_omissions_L.R', 'SIS_memory.thinking_capacities', 'SIS_mood.emotions', 'STAI_S', 'mRNLI_global', 'mRNLI_social', 'PSQI', 'MFI_motivation', 'SCS_Total', 'Age', 'Is_Male', 'Is_Right_Side_Affected', 'Thrombolysis'] and preprocessing scaler\n"
     ]
    }
   ],
   "source": [
    "#Modular approach: possibility to choose the categories of features for the clustering \n",
    "\n",
    "use_connectomes_features = True #pour utiliser les connectomes\n",
    "use_general_features = True  \n",
    "if len(df_T1_corr.columns)>len(df_T3_corr.columns):\n",
    "    features=df_T3_corr.drop(columns=\"Patient\").columns.tolist()\n",
    "else:\n",
    "    features=df_T1_corr.drop(columns=\"Patient\").columns.tolist()\n",
    "#Au début j'ai utilisé les features en gras de la data sur la réduction de dimensions, \n",
    "# mais bon pas ouf les résultats\n",
    "#features=motor_features#+attention_features+executive_features+memory_features+sensory_features+language_features+neglect_features\n",
    "\n",
    "results,summary = main_temporal_pipeline(df_T1, df_T3, df_general_final, features, general_features, \n",
    "                                 algorithms, params, preprocesses, \"T3-T1\", \n",
    "                                 paths_connectomes,\n",
    "                                 use_general_features=use_general_features,\n",
    "                                 use_connectomes_features=use_connectomes_features,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[        Fugl.Meyer_affected_UPPER_EXTREMITY                      \\\n",
       "                                        mean    median       std   \n",
       " Cluster                                                           \n",
       " 0                                  0.089613  0.260048  0.604437   \n",
       " 1                                 -0.293279  0.260048  1.815977   \n",
       " \n",
       "         Ashworth_affected                      \\\n",
       "                      mean    median       std   \n",
       " Cluster                                         \n",
       " 0               -0.086138 -0.289065  1.072821   \n",
       " 1                0.281907  0.581185  0.746702   \n",
       " \n",
       "         Fugl.Meyer_unaffected_UPPER_EXTREMITY                      \\\n",
       "                                          mean    median       std   \n",
       " Cluster                                                             \n",
       " 0                                   -0.053659 -0.021579  0.192482   \n",
       " 1                                    0.175613 -0.021579  2.127449   \n",
       " \n",
       "         Fugl.Meyer_unaffected_WRIST  ...       Age   Is_Male           \\\n",
       "                                mean  ...       std      mean   median   \n",
       " Cluster                              ...                                \n",
       " 0                         -0.152187  ...  0.947032  0.012199  0.58554   \n",
       " 1                          0.498067  ...  1.224732 -0.039923  0.58554   \n",
       " \n",
       "                   Is_Right_Side_Affected                     Thrombolysis  \\\n",
       "               std                   mean    median       std         mean   \n",
       " Cluster                                                                     \n",
       " 0        1.007143              -0.081894  0.823754  1.026909    -0.006479   \n",
       " 1        1.071229               0.268016  0.823754  0.951812     0.021205   \n",
       " \n",
       "                              \n",
       "            median       std  \n",
       " Cluster                      \n",
       " 0       -0.676434  1.024321  \n",
       " 1       -0.676434  1.013428  \n",
       " \n",
       " [2 rows x 93 columns],\n",
       "         Fugl.Meyer_affected_UPPER_EXTREMITY                      \\\n",
       "                                        mean    median       std   \n",
       " Cluster                                                           \n",
       " 0                                  0.089613  0.260048  0.604437   \n",
       " 1                                 -0.293279  0.260048  1.815977   \n",
       " \n",
       "         Ashworth_affected                      \\\n",
       "                      mean    median       std   \n",
       " Cluster                                         \n",
       " 0               -0.086138 -0.289065  1.072821   \n",
       " 1                0.281907  0.581185  0.746702   \n",
       " \n",
       "         Fugl.Meyer_unaffected_UPPER_EXTREMITY                      \\\n",
       "                                          mean    median       std   \n",
       " Cluster                                                             \n",
       " 0                                   -0.053659 -0.021579  0.192482   \n",
       " 1                                    0.175613 -0.021579  2.127449   \n",
       " \n",
       "         Fugl.Meyer_unaffected_WRIST  ...       Age   Is_Male           \\\n",
       "                                mean  ...       std      mean   median   \n",
       " Cluster                              ...                                \n",
       " 0                         -0.152187  ...  0.947032  0.012199  0.58554   \n",
       " 1                          0.498067  ...  1.224732 -0.039923  0.58554   \n",
       " \n",
       "                   Is_Right_Side_Affected                     Thrombolysis  \\\n",
       "               std                   mean    median       std         mean   \n",
       " Cluster                                                                     \n",
       " 0        1.007143              -0.081894  0.823754  1.026909    -0.006479   \n",
       " 1        1.071229               0.268016  0.823754  0.951812     0.021205   \n",
       " \n",
       "                              \n",
       "            median       std  \n",
       " Cluster                      \n",
       " 0       -0.676434  1.024321  \n",
       " 1       -0.676434  1.013428  \n",
       " \n",
       " [2 rows x 93 columns],\n",
       "         Fugl.Meyer_affected_UPPER_EXTREMITY                      \\\n",
       "                                        mean    median       std   \n",
       " Cluster                                                           \n",
       " 0                                 -0.006493  0.260048  1.020989   \n",
       " 1                                  0.298691  0.298691       NaN   \n",
       " \n",
       "         Ashworth_affected                      \\\n",
       "                      mean    median       std   \n",
       " Cluster                                         \n",
       " 0               -0.016839 -0.289065  1.015294   \n",
       " 1                0.774573  0.774573       NaN   \n",
       " \n",
       "         Fugl.Meyer_unaffected_UPPER_EXTREMITY                      \\\n",
       "                                          mean    median       std   \n",
       " Cluster                                                             \n",
       " 0                                   -0.048070 -0.021579  0.966141   \n",
       " 1                                    2.211214  2.211214       NaN   \n",
       " \n",
       "         Fugl.Meyer_unaffected_WRIST  ...      Age   Is_Male           \\\n",
       "                                mean  ...      std      mean   median   \n",
       " Cluster                              ...                               \n",
       " 0                          0.049042  ...  0.99137 -0.012729  0.58554   \n",
       " 1                         -2.255950  ...      NaN  0.585540  0.58554   \n",
       " \n",
       "                   Is_Right_Side_Affected                     Thrombolysis  \\\n",
       "               std                   mean    median       std         mean   \n",
       " Cluster                                                                     \n",
       " 0        1.018165               0.026390  0.823754  1.005477     0.014705   \n",
       " 1             NaN              -1.213954 -1.213954       NaN    -0.676434   \n",
       " \n",
       "                              \n",
       "            median       std  \n",
       " Cluster                      \n",
       " 0       -0.676434  1.016885  \n",
       " 1       -0.676434       NaN  \n",
       " \n",
       " [2 rows x 93 columns]]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# donne la liste des meilleurs features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 Features Showing the Most Difference Between Clusters:\n",
      "Fugl.Meyer_affected_UPPER_EXTREMITY: Z-score difference = 0.00000\n",
      "Ashworth_affected: Z-score difference = 0.00000\n",
      "Fugl.Meyer_unaffected_UPPER_EXTREMITY: Z-score difference = 0.00000\n",
      "Fugl.Meyer_unaffected_WRIST: Z-score difference = 0.00000\n",
      "Fugl.Meyer_unaffected_HAND: Z-score difference = 0.00000\n",
      "Ashworth_unaffected: Z-score difference = 0.00000\n",
      "X9HPT_unaffected: Z-score difference = 0.00000\n",
      "RASP_TOTAL_affected: Z-score difference = 0.00000\n",
      "RASP_TOTAL_unaffected: Z-score difference = 0.00000\n",
      "MOCA_visuo.spatial.executive: Z-score difference = 0.00000\n",
      "MOCA_denomination: Z-score difference = 0.00000\n",
      "MOCA_attention_tot: Z-score difference = 0.00000\n",
      "MOCA_language_tot: Z-score difference = 0.00000\n",
      "MOCA_abstraction: Z-score difference = 0.00000\n",
      "MOCA_memory: Z-score difference = 0.00000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example data: Replace `summary` with your actual cluster summary DataFrame\n",
    "cluster_summary = summary[3]\n",
    "\n",
    "# Flatten the multi-index columns into strings if they are tuples\n",
    "if isinstance(cluster_summary.columns, pd.MultiIndex):\n",
    "    cluster_summary.columns = ['_'.join(col).strip() for col in cluster_summary.columns]\n",
    "\n",
    "# Extract mean and std columns for each feature\n",
    "mean_columns = [col for col in cluster_summary.columns if \"_mean\" in col]\n",
    "std_columns = [col.replace(\"_mean\", \"_std\") for col in mean_columns]\n",
    "# Calculate normalized (Z-score) differences for each feature\n",
    "differences = {}\n",
    "for mean_col, std_col in zip(mean_columns, std_columns):\n",
    "    feature_name = mean_col.replace(\"_mean\", \"\")\n",
    "    \n",
    "    # Mean difference between the two clusters\n",
    "    mean_diff = abs(cluster_summary.loc[0, mean_col] - cluster_summary.loc[1, mean_col])\n",
    "    # Pooled standard deviation\n",
    "    pooled_std = np.sqrt(\n",
    "        (cluster_summary.loc[0, std_col] ** 2 + cluster_summary.loc[1, std_col] ** 2) / 2\n",
    "    )\n",
    "\n",
    "    # Check if pooled_std is valid and avoid division by zero or NaN\n",
    "    if np.isnan(pooled_std) or pooled_std == 0:\n",
    "        z_score = 0  # Assign zero if pooled_std is NaN or zero\n",
    "    else:\n",
    "        z_score = mean_diff / pooled_std\n",
    "    \n",
    "    differences[feature_name] = z_score\n",
    "\n",
    "# Rank features by Z-score differences\n",
    "sorted_features = sorted(differences.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "\n",
    "# Print the ranked features\n",
    "print(\"Top 15 Features Showing the Most Difference Between Clusters:\")\n",
    "for feature, zscore in sorted_features:\n",
    "    print(f\"{feature}: Z-score difference = {zscore:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data vrmt imblanced, possibilité d'utiliser SMOTE\n",
    "- regarder les graph et s'occuper des skewed distributions\n",
    "- on peut changer le threshold de la corr matrix + les features connectomes ou non + les features généraux ou non+\n",
    "le nbr de clusters (2 était tjs le mieux pour moi), les params des modèles, le % de variance que le PCA garde\n",
    "- ultra sensible aux changements des hyper paramètres :("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
