{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = os.getcwd()\n",
    "path_raw_data = os.path.join(cd, r\"data\\TiMeS_Raw_Data2023.xlsx\")\n",
    "file_paths = [\n",
    "    r\".\\data\\Raw_MissingDataImputed\\TiMeS_matrix_mdImputed_allT1.xlsx\",\n",
    "    r\".\\data\\Raw_MissingDataImputed\\TiMeS_matrix_mdImputed_allT2.xlsx\",\n",
    "    r\".\\data\\Raw_MissingDataImputed\\TiMeS_matrix_mdImputed_allT3.xlsx\",\n",
    "    r\".\\data\\Raw_MissingDataImputed\\TiMeS_matrix_mdImputed_allT4.xlsx\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = []\n",
    "for path in file_paths:\n",
    "    df = pd.read_excel(path) \n",
    "    dataframes.append(df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "motor_tests = [\n",
    "    \"Fugl.Meyer_affected_TOTAL\",\n",
    "    \"P.G_affected_FIST_mean\",\n",
    "    \"B.B_blocks_affected_hand\",\n",
    "    \"Purdue_affected_hand\"\n",
    "]\n",
    "\n",
    "qol_tests = [\n",
    "    \"mRS\",\n",
    "    \"Barthel\"\n",
    "]\n",
    "\n",
    "attention_tests = [    \n",
    "    \"TAP_alert_without_warning_RT\",\n",
    "    \"TAP_alert_with_warning_RT\",\n",
    "    \"TAP_divided_attention_single_condition_Auditive_RT\", \n",
    "    \"TAP_divided_attention_single_condition_Visual_RT\", \n",
    "    \"TAP_divided_attention_both_condition_Auditive_RT\",\n",
    "    \"TAP_divided_attention_both_condition_Visual_RT\",\n",
    "    \"Bells_omissions_total.1\",\n",
    "    \"CTM_A_time\"\n",
    "]\n",
    "\n",
    "executive_tests = [\n",
    "    \"Bi.manual_coordination_corrected\",\n",
    "    \"FAB_TOT\", \n",
    "    \"AST_unaffected_TOTAL\", \n",
    "    \"CERAD_copy_TOTAL\",\n",
    "    \"Stroop_interference_time\",\n",
    "    \"Digit_sequencing_TOTAL\",\n",
    "    \"Digit_backward_TOTAL\",\n",
    "    \"Corsi_backward_TOTAL\",\n",
    "    \"CTM_B_time\"\n",
    "]\n",
    "\n",
    "memory_tests = [\n",
    "    \"Corsi_forward_TOTAL\",\n",
    "    \"Digit_forward_TOTAL\"\n",
    "]\n",
    "\n",
    "sensory_test = [\n",
    "    \"RASP_TOTAL_unaffected\"\n",
    "]\n",
    "\n",
    "Language_tests = [\n",
    "    \"Fluency_phon_final_score\",\n",
    "    \"Fluency_sem_final_score\",\n",
    "    \"LAST_TOTAL\"\n",
    "]\n",
    "\n",
    "Neglect_tests = [\n",
    "    \"Line_bissec_20cm\",\n",
    "    \"Line_bissec_.5cm\",\n",
    "    \"Bells_omissions_L.R\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling according to T4 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "columns_for_labeling = [\"Patient\"] + motor_tests + qol_tests + attention_tests + executive_tests + memory_tests + sensory_test + Language_tests + Neglect_tests\n",
    "print(len(columns_for_labeling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataframes = []\n",
    "for df in dataframes:\n",
    "    filtered_df = df[columns_for_labeling] \n",
    "    filtered_dataframes.append(filtered_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for each test\n",
    "thresholds = {\n",
    "    \"Fugl.Meyer_affected_TOTAL\": 50,\n",
    "    \"P.G_affected_FIST_mean\": 18,\n",
    "    \"B.B_blocks_affected_hand\": 40,\n",
    "    \"Purdue_affected_hand\": 12,\n",
    "    \"mRS\": 1,\n",
    "    \"Barthel\": 90, \n",
    "    \"TAP_alert_without_warning_RT\": 400,\n",
    "    \"TAP_alert_with_warning_RT\" : 300,\n",
    "    \"TAP_divided_attention_single_condition_Auditive_RT\" : 450, \n",
    "    \"TAP_divided_attention_single_condition_Visual_RT\" : 400, \n",
    "    \"TAP_divided_attention_both_condition_Auditive_RT\" : 550,\n",
    "    \"TAP_divided_attention_both_condition_Visual_RT\" : 500,\n",
    "    \"Bells_omissions_total.1\" : 6,\n",
    "    \"CTM_A_time\": 60,\n",
    "    \"Bi.manual_coordination_corrected\": 85,\n",
    "    \"FAB_TOT\" : 16, \n",
    "    \"AST_unaffected_TOTAL\" : 15, \n",
    "    \"CERAD_copy_TOTAL\" : 9,\n",
    "    \"Stroop_interference_time\": 90,\n",
    "    \"Digit_sequencing_TOTAL\" : 6,\n",
    "    \"Digit_backward_TOTAL\" : 4,\n",
    "    \"Corsi_backward_TOTAL\" : 4,\n",
    "    \"CTM_B_time\" : 120,\n",
    "    \"Corsi_forward_TOTAL\" : 5,\n",
    "    \"Digit_forward_TOTAL\" : 6,\n",
    "    \"RASP_TOTAL_unaffected\" : 60,\n",
    "    \"Fluency_phon_final_score\" : 15,\n",
    "    \"Fluency_sem_final_score\" : 20,\n",
    "    \"LAST_TOTAL\" : 40,\n",
    "    \"Line_bissec_20cm\" : 2,\n",
    "    \"Line_bissec_.5cm\" : 1,\n",
    "    \"Bells_omissions_L.R\" : 2\n",
    "    \n",
    "}\n",
    "\n",
    "# Create a dataframe with the \"Patient\" column\n",
    "labels = pd.merge(\n",
    "    filtered_dataframes[3],  \n",
    "    filtered_dataframes[2], \n",
    "    on=\"Patient\",           \n",
    "    how=\"outer\", \n",
    "    suffixes=('_T4', '_T3')\n",
    ")       \n",
    "\n",
    "# Calculate the labeling for each test\n",
    "for col, threshold in thresholds.items():\n",
    "    # Generate column names for T4 and T3\n",
    "    t4_col = f\"{col}_T4\"  # Column from T4 after merge\n",
    "    t3_col = f\"{col}_T3\"  # Column from T3 after merge\n",
    "    \n",
    "    # Check if the column exists in the merged DataFrame\n",
    "    if t4_col in labels.columns:\n",
    "        # Take T4 values, fallback to T3 values if T4 is NaN\n",
    "        values = labels[t4_col].fillna(labels[t3_col])\n",
    "        labels[t4_col] = (values >= threshold).astype(int)\n",
    "        if t3_col in labels.columns:\n",
    "            labels.drop(t3_col, axis=1, inplace=True)\n",
    "\n",
    "        #labels.drop(t3_col, axis=1, errors='ignore')\n",
    "    else:\n",
    "        # If T4 column doesn't exist, use T3 values directly\n",
    "        values = labels[t3_col]\n",
    "        labels[t3_col] = (values >= threshold).astype(int)\n",
    "        labels.drop(t4_col)\n",
    "\n",
    "    # verify if the value is higher or lower the threshold\n",
    "    #labels[col] = (values >= threshold).astype(int)\n",
    "\n",
    "# Apply a majority voting\n",
    "labels[\"Recovered\"] = labels.iloc[:, 2:].sum(axis=1) >= (len(thresholds) / 2)\n",
    "\n",
    "# Convert the labels in 1 = Recovered, 0 = not recovered\n",
    "labels[\"Recovered\"] = labels[\"Recovered\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of recovered patients:  54\n",
      "Number of unrecovered patients:  7\n"
     ]
    }
   ],
   "source": [
    "labels = labels[[\"Patient\", \"Recovered\"]]\n",
    "print(\"Number of recovered patients: \", len(labels[labels[\"Recovered\"]==1]))\n",
    "print(\"Number of unrecovered patients: \", len(labels[labels[\"Recovered\"]==0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.to_csv(\"./data/labels.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Fugl.Meyer_affected_TOTAL_T1</th>\n",
       "      <th>P.G_affected_FIST_mean_T1</th>\n",
       "      <th>B.B_blocks_affected_hand_T1</th>\n",
       "      <th>Purdue_affected_hand_T1</th>\n",
       "      <th>mRS_T1</th>\n",
       "      <th>Barthel_T1</th>\n",
       "      <th>TAP_alert_without_warning_RT_T1</th>\n",
       "      <th>TAP_alert_with_warning_RT_T1</th>\n",
       "      <th>...</th>\n",
       "      <th>CTM_B_time_T4</th>\n",
       "      <th>Corsi_forward_TOTAL_T4</th>\n",
       "      <th>Digit_forward_TOTAL_T4</th>\n",
       "      <th>RASP_TOTAL_unaffected_T4</th>\n",
       "      <th>Fluency_phon_final_score_T4</th>\n",
       "      <th>Fluency_sem_final_score_T4</th>\n",
       "      <th>LAST_TOTAL_T4</th>\n",
       "      <th>Line_bissec_20cm_T4</th>\n",
       "      <th>Line_bissec_.5cm_T4</th>\n",
       "      <th>Bells_omissions_L.R_T4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P001</td>\n",
       "      <td>0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>45.333333</td>\n",
       "      <td>48.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>...</td>\n",
       "      <td>78.85</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P002</td>\n",
       "      <td>1</td>\n",
       "      <td>57.0</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>524.0</td>\n",
       "      <td>408.0</td>\n",
       "      <td>...</td>\n",
       "      <td>74.71</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P003</td>\n",
       "      <td>1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>51.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>480.0</td>\n",
       "      <td>...</td>\n",
       "      <td>80.56</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P004</td>\n",
       "      <td>1</td>\n",
       "      <td>47.0</td>\n",
       "      <td>17.666667</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>...</td>\n",
       "      <td>202.60</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P006</td>\n",
       "      <td>1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>26.333333</td>\n",
       "      <td>36.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>363.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>...</td>\n",
       "      <td>164.06</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-2.75</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Patient  Recovered  Fugl.Meyer_affected_TOTAL_T1  P.G_affected_FIST_mean_T1  \\\n",
       "0    P001          0                          58.0                  45.333333   \n",
       "1    P002          1                          57.0                  15.000000   \n",
       "2    P003          1                          60.0                  34.000000   \n",
       "3    P004          1                          47.0                  17.666667   \n",
       "4    P006          1                          54.0                  26.333333   \n",
       "\n",
       "   B.B_blocks_affected_hand_T1  Purdue_affected_hand_T1  mRS_T1  Barthel_T1  \\\n",
       "0                         48.0                     10.0     0.0       100.0   \n",
       "1                         39.0                      5.0     1.0       100.0   \n",
       "2                         51.0                     11.0     0.0       100.0   \n",
       "3                         36.0                      0.0     1.0        95.0   \n",
       "4                         36.0                      5.0     2.0        90.0   \n",
       "\n",
       "   TAP_alert_without_warning_RT_T1  TAP_alert_with_warning_RT_T1  ...  \\\n",
       "0                            245.0                         220.0  ...   \n",
       "1                            524.0                         408.0  ...   \n",
       "2                            345.0                         480.0  ...   \n",
       "3                            285.0                         281.0  ...   \n",
       "4                            363.0                         332.0  ...   \n",
       "\n",
       "   CTM_B_time_T4  Corsi_forward_TOTAL_T4  Digit_forward_TOTAL_T4  \\\n",
       "0          78.85                     9.0                     9.0   \n",
       "1          74.71                     7.0                    11.0   \n",
       "2          80.56                     8.0                    12.0   \n",
       "3         202.60                     9.0                     9.0   \n",
       "4         164.06                     5.0                     8.0   \n",
       "\n",
       "   RASP_TOTAL_unaffected_T4  Fluency_phon_final_score_T4  \\\n",
       "0                     179.0                         13.0   \n",
       "1                     175.0                         13.0   \n",
       "2                     178.0                         15.0   \n",
       "3                     173.0                         16.0   \n",
       "4                     165.0                         19.0   \n",
       "\n",
       "   Fluency_sem_final_score_T4  LAST_TOTAL_T4  Line_bissec_20cm_T4  \\\n",
       "0                        17.0           15.0                  0.5   \n",
       "1                        31.0           15.0                  5.0   \n",
       "2                        20.0           15.0                 -2.0   \n",
       "3                        16.0           15.0                  1.5   \n",
       "4                        16.0           15.0                -12.0   \n",
       "\n",
       "   Line_bissec_.5cm_T4  Bells_omissions_L.R_T4  \n",
       "0                 1.25                     0.0  \n",
       "1                -1.00                    -3.0  \n",
       "2                -1.50                     1.0  \n",
       "3                 0.00                     0.0  \n",
       "4                -2.75                    -2.0  \n",
       "\n",
       "[5 rows x 130 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merged all the sessions in dataframe\n",
    "sessions = [\"T1\", \"T2\", \"T3\", \"T4\"]\n",
    "merged_df = None\n",
    "\n",
    "for session, df in zip(sessions, filtered_dataframes):\n",
    "    \n",
    "    df = df.rename(\n",
    "        columns={col: f\"{col}_{session}\" for col in df.columns if col != \"Patient\"}\n",
    "    )\n",
    "    \n",
    "    if merged_df is None:\n",
    "        merged_df = df\n",
    "    else:\n",
    "        merged_df = pd.merge(merged_df, df, on=\"Patient\", how=\"outer\")\n",
    "\n",
    "# Add the \"Recovered\" label to the merged dataframe\n",
    "# Intersection based on the \"Patient\" column\n",
    "labeled_merged_df = pd.merge(\n",
    "    labels,\n",
    "    merged_df,\n",
    "    on=\"Patient\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Show the resulting dataframe\n",
    "labeled_merged_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate embeddings using Random Forest\n",
    "def generate_random_forest_embeddings(features, labels, sessions, n_estimators=10):\n",
    "    embeddings = []\n",
    "    session_features = {session: [features.columns.get_loc(col) for col in features.columns if session in col] for session in sessions}\n",
    "    \n",
    "    for session, session_indices in session_features.items():\n",
    "        session_data = features.iloc[:, session_indices].values\n",
    "        \n",
    "        rf = RandomForestRegressor(n_estimators=n_estimators, random_state=42)\n",
    "        rf.fit(session_data, labels)\n",
    "        session_embedding = rf.apply(session_data)\n",
    "        embeddings.append(session_embedding)\n",
    "    \n",
    "    return np.stack(embeddings, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate an RNN model\n",
    "def train_rnn_model(embeddings, labels, hidden_size=16, num_layers=2, num_epochs=50, test_size=0.2):\n",
    "    X = torch.tensor(embeddings, dtype=torch.float32)\n",
    "    y = torch.tensor(labels.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "    class RNNClassifier(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, output_size=1):\n",
    "            super(RNNClassifier, self).__init__()\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "            self.fc = nn.Linear(hidden_size, output_size)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        def forward(self, x):\n",
    "            _, (hn, _) = self.rnn(x)\n",
    "            out = self.fc(hn[-1])\n",
    "            return self.sigmoid(out)\n",
    "\n",
    "    input_size = embeddings.shape[2]\n",
    "    model = RNNClassifier(input_size, hidden_size, num_layers)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred_proba = model(X_test)\n",
    "                y_pred = (y_pred_proba >= 0.5).float()\n",
    "\n",
    "                accuracy = accuracy_score(y_test.numpy(), y_pred.numpy())\n",
    "                f1 = f1_score(y_test.numpy(), y_pred.numpy(), zero_division=0)\n",
    "                precision = precision_score(y_test.numpy(), y_pred.numpy(), zero_division=0)\n",
    "                recall = recall_score(y_test.numpy(), y_pred.numpy(), zero_division=0)\n",
    "\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "            print(f\"Accuracy: {accuracy:.4f}, F1-Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Hybrid_lstm_model(X, y, n_time_points, n_epochs=50, hidden_size=16, num_layers=1, test_size=0.2):\n",
    "    import torch.nn as nn  # Import nécessaire dans la fonction si le scope est limité\n",
    "\n",
    "    # Définir la classe dans la fonction\n",
    "    class HybridLSTM(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers):\n",
    "            super(HybridLSTM, self).__init__()\n",
    "            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "            self.fc = nn.Linear(hidden_size, 1)  # Binary output\n",
    "            self.sigmoid = nn.Sigmoid()  # Activation function for binary classification\n",
    "\n",
    "        def forward(self, x):\n",
    "            lstm_out, _ = self.lstm(x)  # lstm_out: (batch_size, time_points, hidden_size)\n",
    "            output = self.fc(lstm_out[:, -1, :])  # Last temporal output\n",
    "            return self.sigmoid(output)\n",
    "\n",
    "    n_patients, _, n_features_per_time_point = X.shape\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)  # Add dimension for binary output\n",
    "\n",
    "    # Divide data into train/test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Initialize the LSTM model\n",
    "    model = HybridLSTM(n_features_per_time_point, hidden_size, num_layers)\n",
    "    criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred_proba = model(X_test)\n",
    "            y_pred = (y_pred_proba >= 0.5).float()\n",
    "\n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test.numpy(), y_pred.numpy())\n",
    "            f1 = f1_score(y_test.numpy(), y_pred.numpy(), zero_division=0)\n",
    "            precision = precision_score(y_test.numpy(), y_pred.numpy(), zero_division=0)\n",
    "            recall = recall_score(y_test.numpy(), y_pred.numpy(), zero_division=0)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")\n",
    "            print(f\"Accuracy: {accuracy:.4f}, F1-Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "features = labeled_merged_df.drop(columns=[\"Patient\", \"Recovered\"]).fillna(0)\n",
    "scaler = StandardScaler()\n",
    "standardized_features = pd.DataFrame(scaler.fit_transform(features), columns=features.columns)\n",
    "\n",
    "sessions = [\"T1\", \"T2\", \"T3\", \"T4\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN model - Training and evaluation WITHOUT SMOTE:\n",
      "Epoch [10/50], Loss: 0.6905\n",
      "Accuracy: 0.9231, F1-Score: 0.9600, Precision: 0.9231, Recall: 1.0000\n",
      "Epoch [20/50], Loss: 0.6452\n",
      "Accuracy: 0.9231, F1-Score: 0.9600, Precision: 0.9231, Recall: 1.0000\n",
      "Epoch [30/50], Loss: 0.5873\n",
      "Accuracy: 0.9231, F1-Score: 0.9600, Precision: 0.9231, Recall: 1.0000\n",
      "Epoch [40/50], Loss: 0.5111\n",
      "Accuracy: 0.9231, F1-Score: 0.9600, Precision: 0.9231, Recall: 1.0000\n",
      "Epoch [50/50], Loss: 0.4340\n",
      "Accuracy: 0.9231, F1-Score: 0.9600, Precision: 0.9231, Recall: 1.0000\n"
     ]
    }
   ],
   "source": [
    "embeddings_no_smote = generate_random_forest_embeddings(standardized_features, labeled_merged_df[\"Recovered\"], sessions)\n",
    "print(\"RNN model - Training and evaluation WITHOUT SMOTE:\")\n",
    "train_rnn_model(embeddings_no_smote, labeled_merged_df[\"Recovered\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid LSTM model - Training and evaluation WITHOUT SMOTE:\n",
      "Epoch 10, Loss: 0.6835\n",
      "Accuracy: 0.9231, F1-Score: 0.9600, Precision: 0.9231, Recall: 1.0000\n",
      "Epoch 20, Loss: 0.6066\n",
      "Accuracy: 0.9231, F1-Score: 0.9600, Precision: 0.9231, Recall: 1.0000\n",
      "Epoch 30, Loss: 0.5219\n",
      "Accuracy: 0.9231, F1-Score: 0.9600, Precision: 0.9231, Recall: 1.0000\n",
      "Epoch 40, Loss: 0.4457\n",
      "Accuracy: 0.9231, F1-Score: 0.9600, Precision: 0.9231, Recall: 1.0000\n",
      "Epoch 50, Loss: 0.3956\n",
      "Accuracy: 0.9231, F1-Score: 0.9600, Precision: 0.9231, Recall: 1.0000\n"
     ]
    }
   ],
   "source": [
    "X = embeddings_no_smote  # Already in (n_patients, n_time_points, n_features_per_time_point) format\n",
    "y = labeled_merged_df[\"Recovered\"].values\n",
    "\n",
    "# Call the function\n",
    "print(\"Hybrid LSTM model - Training and evaluation WITHOUT SMOTE:\")\n",
    "train_Hybrid_lstm_model(X, y, n_time_points=4, n_epochs=50, hidden_size=16, num_layers=1, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add smote to balanced our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN model - Training and evaluation WITHOUT SMOTE:\n",
      "Epoch [10/50], Loss: 0.6962\n",
      "Accuracy: 0.5455, F1-Score: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epoch [20/50], Loss: 0.6878\n",
      "Accuracy: 0.5455, F1-Score: 0.0000, Precision: 0.0000, Recall: 0.0000\n",
      "Epoch [30/50], Loss: 0.6766\n",
      "Accuracy: 0.9091, F1-Score: 0.8889, Precision: 1.0000, Recall: 0.8000\n",
      "Epoch [40/50], Loss: 0.6584\n",
      "Accuracy: 0.7727, F1-Score: 0.8000, Precision: 0.6667, Recall: 1.0000\n",
      "Epoch [50/50], Loss: 0.6256\n",
      "Accuracy: 1.0000, F1-Score: 1.0000, Precision: 1.0000, Recall: 1.0000\n"
     ]
    }
   ],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(standardized_features, labeled_merged_df[\"Recovered\"])\n",
    "X_resampled = pd.DataFrame(X_resampled, columns=standardized_features.columns)\n",
    "\n",
    "embeddings_with_smote = generate_random_forest_embeddings(X_resampled, y_resampled, sessions)\n",
    "print(\"RNN model - Training and evaluation WITHOUT SMOTE:\")\n",
    "train_rnn_model(embeddings_with_smote, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid LSTM model - Training and evaluation WITHOUT SMOTE:\n",
      "Epoch 10, Loss: 0.6434\n",
      "Accuracy: 0.6818, F1-Score: 0.5333, Precision: 0.8000, Recall: 0.4000\n",
      "Epoch 20, Loss: 0.6144\n",
      "Accuracy: 0.7273, F1-Score: 0.6250, Precision: 0.8333, Recall: 0.5000\n",
      "Epoch 30, Loss: 0.5816\n",
      "Accuracy: 0.7727, F1-Score: 0.7059, Precision: 0.8571, Recall: 0.6000\n",
      "Epoch 40, Loss: 0.5473\n",
      "Accuracy: 0.8182, F1-Score: 0.7778, Precision: 0.8750, Recall: 0.7000\n",
      "Epoch 50, Loss: 0.5030\n",
      "Accuracy: 0.8636, F1-Score: 0.8235, Precision: 1.0000, Recall: 0.7000\n"
     ]
    }
   ],
   "source": [
    "X_flattened = embeddings_with_smote.reshape(embeddings_with_smote.shape[0], -1)  # Flatten to 2D for SMOTE\n",
    "y = torch.tensor(y_resampled, dtype=torch.float32).unsqueeze(1)  # Add a dimension for the output\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_flattened, y)\n",
    "\n",
    "# Reshape the resampled data back to (n_patients, n_time_points, n_features_per_time_point)\n",
    "n_time_points = 4\n",
    "n_patients_resampled = X_resampled.shape[0]\n",
    "n_features_per_time_point = X_resampled.shape[1] // n_time_points\n",
    "X_resampled_reshaped = X_resampled.reshape(n_patients_resampled, n_time_points, n_features_per_time_point)\n",
    "\n",
    "print(\"Hybrid LSTM model - Training and evaluation WITHOUT SMOTE:\")\n",
    "train_Hybrid_lstm_model(X_resampled_reshaped, y_resampled, n_time_points=4, n_epochs=50, hidden_size=16, num_layers=1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
